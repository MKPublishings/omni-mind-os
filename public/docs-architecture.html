<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Architecture ‚Äì Omni Mind/OS Documentation</title>
  <link rel="stylesheet" href="/styles/base.css" />
  <link rel="stylesheet" href="/styles/layout.css" />
  <link rel="stylesheet" href="/styles/components.css" />
  <link rel="stylesheet" href="/styles/docs.css" />
</head>
<body class="page">
  <div id="layout">
    <aside id="sidebar">
      <h1 class="logo">Omni Mind/OS</h1>

      <nav class="nav">
        <a href="/index.html" class="nav-link">Home</a>
        <a href="/chat.html" class="nav-link">Chat</a>
        <a href="/docs.html" class="nav-link active">Docs</a>
        <a href="/modes.html" class="nav-link">Modes</a>
        <a href="/settings.html" class="nav-link">Settings</a>
        <a href="/about.html" class="nav-link">About</a>
      </nav>

      <div class="sidebar-footer">
        <span class="status-dot"></span>
        <span class="status-text">System: Online</span>
      </div>
    </aside>

    <main id="main" class="docs-main">
      <div class="docs-breadcrumb">
        <a href="/docs.html" class="breadcrumb-link">‚Üê Documentation Hub</a>
      </div>

      <header class="page-header">
        <h2>Architecture</h2>
        <p class="page-subtitle">System design, request flow, and data storage patterns</p>
      </header>

      <div class="docs-content">
        <section id="overview">
          <h3>System Overview</h3>
          <p>Omni Mind/OS operates as a distributed cognitive pipeline running entirely on Cloudflare's edge network. The architecture follows a three-layer pattern: <strong>Interface</strong> (client-side), <strong>Orchestration</strong> (Worker runtime), and <strong>Intelligence</strong> (AI inference + memory).</p>

          <div class="architecture-diagram">
            <div class="arch-layer">
              <h4>Interface Layer</h4>
              <div class="arch-components">
                <span class="arch-comp">HTML/CSS/JS</span>
                <span class="arch-comp">SSE Client</span>
                <span class="arch-comp">LocalStorage</span>
              </div>
            </div>
            <div class="arch-arrow">‚Üì</div>
            <div class="arch-layer">
              <h4>Orchestration Layer</h4>
              <div class="arch-components">
                <span class="arch-comp">Workers Runtime</span>
                <span class="arch-comp">Mode Selector</span>
                <span class="arch-comp">Request Router</span>
              </div>
            </div>
            <div class="arch-arrow">‚Üì</div>
            <div class="arch-layer">
              <h4>Intelligence Layer</h4>
              <div class="arch-components">
                <span class="arch-comp">Workers AI</span>
                <span class="arch-comp">KV Memory</span>
                <span class="arch-comp">Emotion Engine</span>
              </div>
            </div>
          </div>
        </section>

        <section id="request-flow">
          <h3>Request Flow</h3>
          <ol class="numbered-steps">
            <li>
              <strong>Client Request</strong>
              <p>User submits message via chat interface. JavaScript captures input and initiates SSE connection to <code>/api/omni/stream</code>.</p>
            </li>
            <li>
              <strong>Worker Routing</strong>
              <p>Cloudflare Worker receives POST request, extracts message/mode/model, validates input, and loads session context from KV.</p>
            </li>
            <li>
              <strong>Mode Selection</strong>
              <p>If automatic mode, AI heuristics analyze message intent. If manual, uses user-selected mode. Loads corresponding cognitive primer.</p>
            </li>
            <li>
              <strong>Memory Retrieval</strong>
              <p>Queries MEMORY namespace for relevant facts, preferences, and contextual data. Injects into system prompt.</p>
            </li>
            <li>
              <strong>Emotion Checkpoint</strong>
              <p>Analyzes conversation state, detects emotional tone, modulates response parameters (temperature, length, formality).</p>
            </li>
            <li>
              <strong>AI Inference</strong>
              <p>Streams tokens from Workers AI via model router. Supports fallback chains (primary ‚Üí backup model).</p>
            </li>
            <li>
              <strong>Response Processing</strong>
              <p>Validates output, applies safety guards, chunks into SSE events, streams to client with <code>data:</code> prefix.</p>
            </li>
            <li>
              <strong>Memory Update</strong>
              <p>Extracts learnings from response, updates KV storage with new facts/preferences asynchronously.</p>
            </li>
          </ol>
        </section>

        <section id="data-storage">
          <h3>Data Storage</h3>
          <div class="data-table">
            <table>
              <thead>
                <tr>
                  <th>Namespace</th>
                  <th>Purpose</th>
                  <th>Key Format</th>
                  <th>TTL</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>MEMORY</code></td>
                  <td>Long-term user context, preferences, learned facts</td>
                  <td><code>user:{id}:memory</code></td>
                  <td>Persistent</td>
                </tr>
                <tr>
                  <td><code>MIND</code></td>
                  <td>Session state, conversation history, mode cache</td>
                  <td><code>session:{id}</code></td>
                  <td>7 days</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h4>Memory Schema</h4>
          <div class="code-block">
            <pre><code>interface MemoryEntry {
  id: string;              // Unique identifier
  type: 'fact' | 'preference' | 'context';
  content: string;         // Memory content
  timestamp: number;       // Unix timestamp
  confidence: number;      // 0-1 reliability score
  tags: string[];          // Categorization tags
  links: string[];         // Related memory IDs
}</code></pre>
          </div>

          <h4>Session Schema</h4>
          <div class="code-block">
            <pre><code>interface Session {
  id: string;
  userId: string;
  messages: Message[];
  currentMode: CognitiveMode;
  emotionalState: EmotionalProfile;
  createdAt: number;
  lastActive: number;
}</code></pre>
          </div>
        </section>

        <section id="model-routing">
          <h3>Model Routing</h3>
          <p>The model router supports multiple AI providers with automatic fallback chains to ensure reliability.</p>

          <h4>Router Logic</h4>
          <div class="code-block">
            <pre><code>async function routeModel(model: string, env: Env) {
  const modelMap = {
    'omni': env.MODEL_OMNI || '@cf/meta/llama-3.3-70b-instruct-fp8-fast',
    'gpt-4o': env.MODEL_GPT4O || '@cf/openai/gpt-4o',
    'gpt-4o-mini': env.MODEL_GPT4O_MINI || '@cf/openai/gpt-4o-mini',
    'deepseek': env.MODEL_DEEPSEEK || '@cf/deepseek-ai/deepseek-r1-distill-llama-70b'
  };

  const actualModel = modelMap[model] || modelMap['omni'];

  try {
    return await env.AI.run(actualModel, payload);
  } catch (error) {
    // Fallback to Llama if primary fails
    return await env.AI.run(modelMap['omni'], payload);
  }
}</code></pre>
          </div>

          <h4>Supported Models</h4>
          <ul>
            <li><strong>Omni (Default)</strong> ‚Äì Meta Llama 3.3 70B FP8, optimized for speed and reasoning</li>
            <li><strong>GPT-4o</strong> ‚Äì OpenAI's multimodal flagship, best for complex tasks</li>
            <li><strong>GPT-4o Mini</strong> ‚Äì Faster, cost-efficient version for simple queries</li>
            <li><strong>DeepSeek R1</strong> ‚Äì Specialized for reasoning-heavy workloads</li>
          </ul>
        </section>

        <section id="streaming">
          <h3>Streaming Protocol</h3>
          <p>Omni Mind/OS uses Server-Sent Events (SSE) for real-time token streaming without WebSocket overhead.</p>

          <h4>SSE Event Format</h4>
          <div class="code-block">
            <pre><code>data: {"type": "token", "content": "Hello"}
data: {"type": "token", "content": " world"}
data: {"type": "metadata", "mode": "analyst", "model": "omni"}
data: {"type": "done"}</code></pre>
          </div>

          <h4>Client-Side Handling</h4>
          <div class="code-block">
            <pre><code>const eventSource = new EventSource('/api/omni/stream');

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.type === 'token') {
    appendToChat(data.content);
  } else if (data.type === 'done') {
    eventSource.close();
    finalizeMessage();
  }
};</code></pre>
          </div>

          <h4>Token Budgeting</h4>
          <p>Adaptive output token allocation based on response length:</p>
          <ul>
            <li>Simple queries: 512 tokens</li>
            <li>Standard responses: 2048 tokens</li>
            <li>Complex explanations: 4096 tokens</li>
            <li>Deep analysis: 8192 tokens</li>
          </ul>
        </section>

        <section id="emotion-engine">
          <h3>Emotion Engine</h3>
          <p>Analyzes conversation tone and dynamically modulates response characteristics.</p>

          <h4>Detection Heuristics</h4>
          <div class="code-block">
            <pre><code>interface EmotionalProfile {
  valence: number;      // -1 (negative) to 1 (positive)
  arousal: number;      // 0 (calm) to 1 (excited)
  formality: number;    // 0 (casual) to 1 (formal)
  urgency: number;      // 0 (relaxed) to 1 (urgent)
}

function detectEmotion(message: string): EmotionalProfile {
  const urgentKeywords = ['asap', 'urgent', 'quickly', 'now'];
  const formalMarkers = ['please', 'kindly', 'would you'];
  const excitement = (message.match(/[!?]{2,}/g) || []).length;

  return {
    valence: analyzesentiment(message),
    arousal: excitement / 10,
    formality: formalMarkers.some(m => message.includes(m)) ? 0.8 : 0.3,
    urgency: urgentKeywords.some(k => message.includes(k)) ? 0.9 : 0.2
  };
}</code></pre>
          </div>

          <h4>Response Modulation</h4>
          <ul>
            <li><strong>High Urgency</strong> ‚Üí Lower temperature (0.3), concise responses</li>
            <li><strong>Formal Tone</strong> ‚Üí Structured formatting, professional language</li>
            <li><strong>Excited State</strong> ‚Üí Higher temperature (0.8), creative responses</li>
            <li><strong>Negative Valence</strong> ‚Üí Empathetic framing, supportive tone</li>
          </ul>
        </section>

        <section id="safety">
          <h3>Safety & Validation</h3>
          
          <h4>Input Validation</h4>
          <ul>
            <li>Message length: 1-50,000 characters</li>
            <li>Rate limiting: 60 requests/minute per session</li>
            <li>Content filtering: Block malicious prompts, PII extraction attempts</li>
          </ul>

          <h4>Output Guards</h4>
          <div class="code-block">
            <pre><code>function validateOutput(response: string): ValidationResult {
  const checks = [
    checkForPII(response),
    checkForHarmfulContent(response),
    checkForBrokenMarkdown(response),
    checkForRepetition(response)
  ];

  return {
    safe: checks.every(c => c.passed),
    warnings: checks.filter(c => !c.passed).map(c => c.message)
  };
}</code></pre>
          </div>

          <h4>Error Handling</h4>
          <ul>
            <li><strong>AI Timeout</strong> ‚Üí Fallback to backup model, retry with reduced max_tokens</li>
            <li><strong>KV Failure</strong> ‚Üí Degrade gracefully to stateless mode</li>
            <li><strong>Network Errors</strong> ‚Üí Client-side retry with exponential backoff</li>
          </ul>
        </section>

        <section id="performance">
          <h3>Performance Optimization</h3>
          
          <h4>Edge Caching</h4>
          <ul>
            <li>Static assets cached at CDN edge (24-hour TTL)</li>
            <li>Mode primers cached in Worker memory after first load</li>
            <li>KV reads use stale-while-revalidate pattern</li>
          </ul>

          <h4>Latency Breakdown</h4>
          <table>
            <thead>
              <tr>
                <th>Stage</th>
                <th>Typical Latency</th>
                <th>Optimization</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>DNS Lookup</td>
                <td>~5ms</td>
                <td>Cloudflare Anycast</td>
              </tr>
              <tr>
                <td>TLS Handshake</td>
                <td>~15ms</td>
                <td>ECDSA certificates</td>
              </tr>
              <tr>
                <td>Worker Cold Start</td>
                <td>0ms</td>
                <td>V8 isolates (no cold starts)</td>
              </tr>
              <tr>
                <td>KV Read</td>
                <td>~20ms</td>
                <td>Eventually consistent reads</td>
              </tr>
              <tr>
                <td>First Token</td>
                <td>~50-100ms</td>
                <td>Streaming inference</td>
              </tr>
            </tbody>
          </table>

          <h4>Bandwidth Efficiency</h4>
          <ul>
            <li>SSE streaming: ~2KB overhead vs WebSocket's ~4KB</li>
            <li>Gzip compression on text responses</li>
            <li>Incremental DOM updates (avoid full re-renders)</li>
          </ul>
        </section>

        <section id="next-steps">
          <h3>Explore More</h3>
          <div class="nav-cards">
            <a href="/docs-modes.html" class="nav-card">
              <span class="nav-icon">üß†</span>
              <div>
                <strong>Modes</strong>
                <p>Cognitive primitives and selection heuristics</p>
              </div>
              <span class="nav-arrow">‚Üí</span>
            </a>
            <a href="/docs-memory.html" class="nav-card">
              <span class="nav-icon">üíæ</span>
              <div>
                <strong>Memory</strong>
                <p>Persistent context and session management</p>
              </div>
              <span class="nav-arrow">‚Üí</span>
            </a>
            <a href="/docs-api.html" class="nav-card">
              <span class="nav-icon">‚ö°</span>
              <div>
                <strong>API Reference</strong>
                <p>Endpoints, authentication, and examples</p>
              </div>
              <span class="nav-arrow">‚Üí</span>
            </a>
          </div>
        </section>
      </div>

      <button id="scroll-top" class="scroll-top-btn" aria-label="Scroll to top">‚Üë</button>
    </main>
  </div>

  <script src="/scripts/docs.js"></script>
</body>
</html>
